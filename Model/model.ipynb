{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.data.tables import TableServiceClient\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã€€Setup connection to Azure Table Storage by its connection string \n",
    "connection_string = os.environ.get(\"AZURE_TABLE_STORAGE_CONNECTION_STRING\")\n",
    "table_service_client = TableServiceClient.from_connection_string(conn_str=connection_string)\n",
    "\n",
    "# Get table that stored FYP player data\n",
    "table_client = table_service_client.get_table_client(table_name=\"fyptablestorage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entites from Azure Table and convert to pandas dataframe\n",
    "entities = table_client.query_entities(\"PartitionKey eq 'FYP'\")\n",
    "df = pd.DataFrame(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_list = [f\"Landmark{i}\" for i in range(12, 30) if i not in [17, 18, 19, 20, 21, 22]]\n",
    "df = df[[\"Age\", \"Score\", *landmark_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert string representation of list to actual list\n",
    "def str_to_list(landmark_str):\n",
    "    return ast.literal_eval(landmark_str)\n",
    "\n",
    "\n",
    "# Function to calculate motion entropy (G(Y)) for each landmark using log base 2\n",
    "def calculate_motion_entropy(motion_amplitudes):\n",
    "    Dz = np.sum(motion_amplitudes)\n",
    "    if Dz == 0:\n",
    "        return np.nan\n",
    "    entropy = -np.sum([(D/Dz) * np.log2(D/Dz) for D in motion_amplitudes if D > 0])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Function to calculate overall entropy features\n",
    "def calculate_overall_entropy_features(row, landmark_columns):\n",
    "    entropies = []\n",
    "    for col in landmark_columns:\n",
    "        entropy = calculate_motion_entropy(row[col])\n",
    "        if not np.isnan(entropy):\n",
    "            entropies.append(entropy)\n",
    "    \n",
    "    if len(entropies) == 0:\n",
    "        return pd.Series([np.nan] + [np.nan] * len(landmark_columns) + [np.nan], \n",
    "                         index=[\"mean_entropy\"] + [f\"entropy_{col}\" for col in landmark_columns] + [\"aggregated_entropy\"])\n",
    "    \n",
    "    mean_entropy = np.mean(entropies)\n",
    "    \n",
    "    # Aggregating all motion amplitudes across landmarks and calculating entropy of the aggregated data\n",
    "    aggregated_motion_amplitudes = []\n",
    "    for col in landmark_columns:\n",
    "        aggregated_motion_amplitudes.extend(row[col])\n",
    "    \n",
    "    aggregated_Dz = np.sum(aggregated_motion_amplitudes)\n",
    "    if aggregated_Dz == 0:\n",
    "        aggregated_entropy = np.nan\n",
    "    else:\n",
    "        aggregated_entropy = -np.sum([(D/aggregated_Dz) * np.log2(D/aggregated_Dz) for D in aggregated_motion_amplitudes if D > 0])\n",
    "    \n",
    "    return pd.Series([mean_entropy] + entropies + [aggregated_entropy], \n",
    "                     index=[\"mean_entropy\"] + [f\"entropy_{col}\" for col in landmark_columns] + [\"aggregated_entropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all Landmark columns from string to list\n",
    "for landmark in landmark_list:\n",
    "    df[landmark] = df[landmark].apply(str_to_list)\n",
    "\n",
    "# Applying the function to calculate the overall entropy features\n",
    "entropy_features_df = df.apply(calculate_overall_entropy_features, axis=1, landmark_columns=landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[[\"Score\"]]\n",
    "# Combining the entropy features with the original DataFrame\n",
    "final_df = pd.concat([test_df, entropy_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, ConstantKernel as C\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(columns=[\"Score\"])\n",
    "y = df[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianProcessRegressor()\n",
    "\n",
    "# Define the model and hyperparameters to search\n",
    "model = GaussianProcessRegressor()\n",
    "\n",
    "# Define the parameter grid with expanded hyperparameters\n",
    "param_grid = {\n",
    "    \"kernel\": [\n",
    "        C(1.0, (1e-2, 1e2)) * RBF(length_scale, (1e-2, 1e2)) for length_scale in [1, 10, 100]\n",
    "    ] + [\n",
    "        C(1.0, (1e-2, 1e2)) * Matern(length_scale, (1e-2, 1e2), nu) for length_scale in [1, 10, 100] for nu in [0.5, 1.5, 2.5]\n",
    "    ] + [\n",
    "        C(1.0, (1e-2, 1e2)) * RationalQuadratic(length_scale, alpha) for length_scale in [1, 10, 100] for alpha in [0.1, 1.0, 10.0]\n",
    "    ] + [\n",
    "        C(1.0, (1e-2, 1e2)) * ExpSineSquared(length_scale, periodicity) for length_scale in [1, 10, 100] for periodicity in [1.0, 3.0]\n",
    "    ] + [\n",
    "        C(1.0, (1e-2, 1e2)) * DotProduct(sigma_0=sigma) for sigma in [0.1, 1.0, 10.0]\n",
    "    ],\n",
    "    \"alpha\": [1e-5, 1e-3, 1e-1, 1.0, 10.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Perform GridSearchCV with LOOCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model using LOOCV\n",
    "results = {\"mse\": [], \"mae\": []}\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    results[\"mse\"].append(mse)\n",
    "    results[\"mae\"].append(mae)\n",
    "\n",
    "# Calculate the average performance metrics\n",
    "avg_mse = np.mean(results[\"mse\"])\n",
    "avg_mae = np.mean(results[\"mae\"])\n",
    "avg_rmse = np.mean(results[\"rmse\"])\n",
    "\n",
    "print(f\"GaussianProcessRegressor: Mean Squared Error = {avg_mse:.4f}, Mean Absolute Error = {avg_mae:.4f}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Save the best model to PKL format\n",
    "with open(f\"{os.path.dirname(os.path.abspath(''))}\\\\ContainerizedModelFlaskAPI\\\\depresson_gpr_model.txt\", 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "print(\"Best model exported to depresson_gpr_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
